{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (2.28.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: geopy in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\asillers\\appdata\\local\\anaconda3\\lib\\site-packages (from geopy) (2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#scrape urban turf\n",
    "%pip install pandas\n",
    "%pip install requests\n",
    "import requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install geopy\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_lat_long(address):\\n    geolocator = Nominatim(user_agent=\"my_geocoder_app\")\\n    time.sleep(1.1)\\n    location = geolocator.geocode(address)\\n\\n    if location:\\n        return location.latitude, location.longitude\\n    else:\\n        return None\\n    \\nurban_turf[\\'Latitude\\'] = \"\"\\nurban_turf[\\'Longitude\\'] = \"\"\\n\\nfor i in range(0, len(urban_turf)):\\n    try:\\n        c = urban_turf[\\'Address:\\'][i]\\n        d = re.sub(\"-[\\\\d]*\", \"\", c)\\n        a = get_lat_long(d)\\n        urban_turf[\\'Latitude\\'][i] =  a[0]\\n        urban_turf[\\'Longaitude\\'][i] =  a[1]\\n\\n    except Exception:\\n        pass\\nurban_turf[\\'No. of units:\\'] = pd.to_numeric(urban_turf[\\'No. of units:\\'], errors=\\'coerce\\')\\n\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get(\n",
    "    \"https://dc.urbanturf.com/pipeline?search_string=&project_type=0&status=0&more_or_less=0&number_of_units=&number_of_units_selector=0&city=0&state=0&zip=0&order_by=last_updated&direction=desc&filtered=Yes&limit=1000\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "page_body = soup.body\n",
    "\n",
    "labels = [item for item in page_body.findAll(\"p\")]\n",
    "list_lab = [i.text.strip() for i in labels][1:] \n",
    "list_lab = [word.replace('\\n','') for word in list_lab]\n",
    "\n",
    "torep = {\"Location:            \":\"\", \"Project type:            \":\"\",\"Status:            \":\"\", \"Size:            \":\"\"} #titles of list\n",
    "torep = dict((re.escape(k), v) for k, v in torep.items()) #turn into dictionary using 'torep' value as keys\n",
    "pattern = re.compile(\"|\".join(torep.keys())) #joins with \"|\"\n",
    "\n",
    "desc = []\n",
    "\n",
    "for i in range(0, len(list_lab)):\n",
    "    a = list_lab[i] #first value in list\n",
    "    text = pattern.sub(lambda m:torep[re.escape(m.group(0))], a) #substitute the pattern in each value of the list with \"\"\n",
    "    desc.append(text)\n",
    "\n",
    "seq = range(0, len(desc))\n",
    "\n",
    "urban_turf= pd.DataFrame(\n",
    "    {\"Address\": [desc[i] for i in seq[0::5]],\n",
    "     \"Neighborhood\": [desc[i] for i in seq[1::5]],\n",
    "     \"Project.Type\":[desc[i] for i in seq[2::5]],\n",
    "     \"Status\":[desc[i] for i in seq[3::5]],\n",
    "     \"Size\":[desc[i] for i in seq[4::5]],\n",
    "     \"Pipeline Link\": None,\n",
    "     'Neighborhood:':None,\n",
    "     'Pipeline':None,\n",
    "     'Project type:': None, \n",
    "     'No. of units:': None, \n",
    "     'Types of units:': None, \n",
    "     'Unit sizes:':None, \n",
    "     'Amenities:':None, \n",
    "     'Pricing:':None, \n",
    "     'Website:':None, \n",
    "     'Architect:':None, \n",
    "     'Last updated:':None, \n",
    "     'Address:':None, \n",
    "     'Status:':None, \n",
    "     'First move-ins:':None, \n",
    "     \"Latitude\":None, \n",
    "     \"Longitude\":None})    \n",
    "\n",
    "#can I get the pipeline link from scraping above?\n",
    "\n",
    "def search_pipe_link(row):\n",
    "    ut_address_col = str(row['Address'])\n",
    "    projecttype_col = str(row['Project.Type'])\n",
    "    status_col = str(row['Status'])\n",
    "    toreplace = \"&more_or_less=0&number_of_units=&number_of_units_selector=0&city=0&state=0&zip=0&order_by=last_updated&direction=desc&filtered=Yes\"\n",
    "    time.sleep(1.25)\n",
    "    searchlink = \"https://dc.urbanturf.com/pipeline?search_string=\" + ut_address_col.replace(\" \", \"_\" ) + \"&project_type=\" + projecttype_col.replace(\" \", \"+\").replace(\"&\", \"%26\") + \"&status=\" + status_col.replace(\",\", \"%2C\").replace(\" \", \"+\") + toreplace\n",
    "    page = requests.get(searchlink)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    page_body = soup.body\n",
    "    table = page_body.find_all('div', class_ = 'pipeline-item clickable')\n",
    "    if table:\n",
    "        tag = table[0].find('a').get('href')\n",
    "        return tag\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "urban_turf['Pipeline Link'] = urban_turf.apply(search_pipe_link, axis=1)\n",
    "urban_turf = urban_turf.dropna(subset=['Pipeline Link']) #would like to encorporate into function\n",
    "urban_turf = urban_turf.reset_index(drop=True)\n",
    "column_names = urban_turf.columns\n",
    "\n",
    "urban_turf.loc[urban_turf['Address'] == \"The MO\", 'Pipeline Link'] = \"https://dc.urbanturf.com/pipeline/715/The_MO\"\n",
    "urban_turf.loc[urban_turf['Address'] == \"Valo\", 'Pipeline Link'] = \"https://dc.urbanturf.com/pipeline/551/Valo\"\n",
    "\n",
    "def scrape_pipelinelink(row):\n",
    "    page = requests.get(row[\"Pipeline Link\"])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find_all('div', class_ = 'block') \n",
    "    txt = str(table)\n",
    "    txt = txt.replace('\\r', '').replace('\\n', '')\n",
    "    txt = txt.split(\"<p>\")\n",
    "    txt = [x for x in txt if '<span class=\"label\">' in x]\n",
    "    txt = [txt.split(\"</span>\") for txt in txt]\n",
    "\n",
    "    key = [txt[i][0] for i in list(range(0,len(txt)))]\n",
    "    txt = [txt[i][1] for i in list(range(0,len(txt)))]\n",
    "    key = [x.replace('<span class=\"label\">', \"\") for x in key]\n",
    "    df_dictionary = pd.DataFrame([ dict(zip(key, txt))])\n",
    "\n",
    "    for key, txt in df_dictionary.items():\n",
    "        if key in column_names:\n",
    "            row[key] = df_dictionary[key][0]\n",
    "\n",
    "urban_turf.apply(scrape_pipelinelink, axis=1)\n",
    "\n",
    "urban_turf['No. of units:'] = [x.replace(\" \", \"\").replace(\"</p>\", \"\") for x in urban_turf['No. of units:']]\n",
    "urban_turf.loc[urban_turf['Architect:'].notnull(), 'Architect:'] = [x.replace(\"\\t\", \"\").replace(\"</p>\", \"\") for x in urban_turf.loc[urban_turf['Architect:'].notnull(), 'Architect:']]\n",
    "urban_turf.loc[urban_turf['Website:'].notnull(), 'Website:'] = [re.search(r'\".*\\\"\\sstyle', x).group(0).replace('\" style', \"\").replace('\"', \"\") for x in urban_turf.loc[urban_turf['Website:'].notnull(), 'Website:']]\n",
    "urban_turf.loc[urban_turf['Status:'].notnull(), 'Status:'] =  [re.sub(r'</p>.*', \"\", x) for x in urban_turf.loc[urban_turf['Status:'].notnull(), 'Status:']]\n",
    "urban_turf.loc[urban_turf['Address:'].notnull(), 'Address:'] =  [re.sub(r'</p>.*', \"\", x) for x in urban_turf.loc[urban_turf['Address:'].notnull(), 'Address:']]\n",
    "\n",
    "substitutions = [\n",
    "    (r'<br/>', ''),\n",
    "    (r'\\xa0', ' '),\n",
    "    (r'&amp;', '&'),\n",
    "    #(r' +', \"\")\n",
    "]\n",
    "\n",
    "def replace_text(row):\n",
    "    for old_pattern, new_string in substitutions:\n",
    "        row['Address:'] = re.sub(old_pattern, new_string, row['Address:'])\n",
    "\n",
    "urban_turf.apply(replace_text, axis=1)\n",
    "\n",
    "urban_turf['Address:'] = [re.sub(r'( {2,})(\\.*)( {2,})(\\.*)', r'\\2,\\3\\4', x) for x in urban_turf['Address:']] #why are there commas on each side?\n",
    "\n",
    "def get_lat_long(address):\n",
    "    geolocator = Nominatim(user_agent=\"my_geocoder_app\")\n",
    "    time.sleep(1.1)\n",
    "    location = geolocator.geocode(address)\n",
    "\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "for i in range(0, len(urban_turf)):\n",
    "    try:\n",
    "        c = urban_turf['Address:'][i]\n",
    "        d = re.sub(\"-[\\d]*\", \"\", c)\n",
    "        a = get_lat_long(d)\n",
    "        urban_turf['Latitude'][i] =  a[0]\n",
    "        urban_turf['Longitude'][i] =  a[1]\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "urban_turf['No. of units:'] = pd.to_numeric(urban_turf['No. of units:'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
